{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3b3134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, shutil, torch, cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models import vgg16\n",
    "from torchvision import transforms as T\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e302dc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = r\"C:\\Users\\atole\\OneDrive\\Desktop\\Python\\archive(1)\\Image Super Resolution - Unsplash\\high res\"\n",
    "target_root = r\"\\Users\\atole\\OneDrive\\Desktop\\Python\\Working Dataset\"\n",
    "os.makedirs(target_root, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d7e9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, cv2, shutil, random\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "source_dir= r\"C:\\Users\\atole\\OneDrive\\Desktop\\Python\\archive(1)\\Image Super Resolution - Unsplash\\high res\"        \n",
    "target_root= r\"C:\\Users\\atole\\OneDrive\\Desktop\\Python\\Working Dataset\"\n",
    "valid_images= []\n",
    "\n",
    "print(\" Filtering images divisible by 8...\")\n",
    "\n",
    "\n",
    "for fname in tqdm(os.listdir(source_dir)):\n",
    "    if fname.lower().endswith(('.jpg', '.png')):\n",
    "        fpath = os.path.join(source_dir, fname)\n",
    "        img = cv2.imread(fpath)\n",
    "        if img is None:\n",
    "            continue\n",
    "        h, w = img.shape[:2]\n",
    "        if h % 8 == 0 and w % 8 == 0:\n",
    "            valid_images.append(fname)\n",
    "\n",
    "print(f\" Valid images: {len(valid_images)}\")\n",
    "\n",
    "\n",
    "random.seed(42)\n",
    "train_imgs, test_imgs = train_test_split(valid_images, test_size=0.2, random_state=42)\n",
    "train_A, train_B = train_test_split(train_imgs, test_size=0.5, random_state=42)\n",
    "showcase_imgs = random.sample(test_imgs, min(50, len(test_imgs)))\n",
    "\n",
    "def extract_patches(img, patch_size=256, stride=256):\n",
    "    patches = []\n",
    "    h, w = img.shape[:2]\n",
    "    for y in range(0, h - patch_size + 1, stride):\n",
    "        for x in range(0, w - patch_size + 1, stride):\n",
    "            patch = img[y:y+patch_size, x:x+patch_size]\n",
    "            patches.append(patch)\n",
    "    return patches\n",
    "\n",
    "def save_patches_from_list(file_list, folder_name, patch_size=256, stride=256):\n",
    "    dest = os.path.join(target_root, folder_name)\n",
    "    if os.path.exists(dest):\n",
    "        shutil.rmtree(dest)\n",
    "    os.makedirs(dest, exist_ok=True)\n",
    "\n",
    "    patch_id = 0\n",
    "    for fname in tqdm(file_list, desc=f\" Creating patches for {folder_name}\"):\n",
    "        img = cv2.imread(os.path.join(source_dir, fname))\n",
    "        if img is None:\n",
    "            continue\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        h, w = img.shape[:2]\n",
    "        h, w = h - (h % 8), w - (w % 8)\n",
    "        img = cv2.resize(img, (w, h))\n",
    "        patches = extract_patches(img, patch_size=patch_size, stride=stride)\n",
    "        for patch in patches:\n",
    "            out_path = os.path.join(dest, f\"{patch_id:06}.png\")\n",
    "            cv2.imwrite(out_path, cv2.cvtColor(patch, cv2.COLOR_RGB2BGR))\n",
    "            patch_id += 1\n",
    "\n",
    "def copy_images(file_list, folder_name):\n",
    "    dest = os.path.join(target_root, folder_name)\n",
    "    if os.path.exists(dest):\n",
    "        shutil.rmtree(dest)\n",
    "    os.makedirs(dest, exist_ok=True)\n",
    "\n",
    "    for fname in tqdm(file_list, desc=f\" Copying full images to {folder_name}\"):\n",
    "        src_path = os.path.join(source_dir, fname)\n",
    "        dst_path = os.path.join(dest, fname)\n",
    "        shutil.copy(src_path, dst_path)\n",
    "            \n",
    "save_patches_from_list(train_A, \"train_A\", patch_size=256, stride=256)   \n",
    "save_patches_from_list(train_B, \"train_B\", patch_size=256, stride=256)\n",
    "copy_images(test_imgs, \"test\")\n",
    "copy_images(showcase_imgs, \"showcase\")\n",
    "\n",
    "print(\"\\n Dataset Preparation Complete:\")\n",
    "print(f\" Train A: {len(os.listdir(os.path.join(target_root, 'train_A')))} patches\")\n",
    "print(f\" Train B: {len(os.listdir(os.path.join(target_root, 'train_B')))} patches\")\n",
    "print(f\" Test: {len(os.listdir(os.path.join(target_root, 'test')))} full images\")\n",
    "print(f\" Showcase: {len(os.listdir(os.path.join(target_root, 'showcase')))} full images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e243d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f992de9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = \"https://raw.githubusercontent.com/swz30/Restormer/main/basicsr/models/archs/restormer_arch.py\"\n",
    "save_path = \"restormer_arch.py\"\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"Saved to {save_path}\")\n",
    "else:\n",
    "    print(f\" Failed to download. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aaf2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!pip install einops\n",
    "sys.path.append(r'C:\\Users\\atole\\OneDrive\\Desktop\\Python\\Working Dataset')\n",
    "from restormer_arch import Restormer\n",
    "import torch\n",
    "\n",
    "teacher = Restormer(\n",
    "    inp_channels=3,               \n",
    "    out_channels=3,\n",
    "    dim=48,\n",
    "    num_blocks=[4, 6, 6, 8],\n",
    "    num_refinement_blocks=4,\n",
    "    heads=[1, 2, 4, 8],\n",
    "    ffn_expansion_factor=2.66,\n",
    "    bias=False,\n",
    "    LayerNorm_type='WithBias',\n",
    "    dual_pixel_task=False        \n",
    ")\n",
    "\n",
    "\n",
    "weights = torch.load(r\"C:\\Users\\atole\\OneDrive\\Desktop\\Python\\single_image_defocus_deblurring.pth\", map_location=device)\n",
    "teacher.load_state_dict(weights.get(\"params\", weights))\n",
    "teacher = teacher.to(device).eval()\n",
    "\n",
    "print(\" Restormer (3-channel teacher for single-image defocus deblurring) ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1098b55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentCNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, 32, 3, padding=1), torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(32, 64, 3, padding=1), torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "            torch.nn.Conv2d(64, 64, 3, padding=1), torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(64, 64, 3, padding=1), torch.nn.ReLU(),\n",
    "            torch.nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        )\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(64, 32, 3, padding=1), torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(32, 3, 3, padding=1), torch.nn.Tanh()  \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return (x + 1) / 2\n",
    "\n",
    "student = StudentCNN().to(device)\n",
    "optimizer = torch.optim.Adam(student.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fb9556",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = vgg16(pretrained=True).features.to(device).eval()\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "vgg_normalize = T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225])\n",
    "\n",
    "def perceptual_loss(pred, target):\n",
    "    pred_resized = F.interpolate(pred, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "    target_resized = F.interpolate(target, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "    pred_norm = vgg_normalize(pred_resized.squeeze(0)).unsqueeze(0)\n",
    "    target_norm = vgg_normalize(target_resized.squeeze(0)).unsqueeze(0)\n",
    "    pred_feat = vgg(pred_norm)\n",
    "    target_feat = vgg(target_norm)\n",
    "    return F.l1_loss(pred_feat, target_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba09b4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(target_root, \"train_B\")  \n",
    "train_files = os.listdir(train_path)\n",
    "student.train()\n",
    "teacher.eval()\n",
    "best_ssim = 0.0\n",
    "\n",
    "for epoch in range(15):\n",
    "    total_loss = 0\n",
    "    student.train()\n",
    "    ssim_total = 0.0\n",
    "\n",
    "    for fname in tqdm(train_files, desc=f\" Training Student | Epoch {epoch+1}\"):\n",
    "        img = cv2.imread(os.path.join(train_path, fname))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        h, w = img.shape[:2]\n",
    "        h, w = h - (h % 8), w - (w % 8)\n",
    "        gt = cv2.resize(img, (w, h))\n",
    "        lr = cv2.resize(gt, (w//2, h//2), interpolation=cv2.INTER_LINEAR)\n",
    "        input_img = cv2.resize(lr, (w, h), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "        input_tensor = to_tensor(input_img).unsqueeze(0).to(device)\n",
    "        gt_tensor = to_tensor(gt).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            target_tensor = teacher(input_tensor)\n",
    "\n",
    "        output = student(input_tensor)\n",
    "\n",
    "        l1 = criterion(output, target_tensor)\n",
    "        gt_l1 = criterion(output, gt_tensor)\n",
    "        p_loss = perceptual_loss(output, gt_tensor)\n",
    "        loss = 0.9 * l1 + 0.1 * gt_l1 + 0.005 * p_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            s_output = output.squeeze(0).clamp(0, 1).cpu().numpy()\n",
    "            s_gt = to_tensor(gt).numpy()\n",
    "            s_output = np.transpose(s_output, (1, 2, 0))\n",
    "            s_gt = np.transpose(s_gt, (1, 2, 0))\n",
    "            score = ssim(s_output, s_gt, channel_axis=2, data_range=1.0, win_size=11)\n",
    "            ssim_total += score\n",
    "\n",
    "    avg_loss = total_loss / len(train_files)\n",
    "    avg_ssim = ssim_total / len(train_files)\n",
    "    print(f\" Epoch {epoch+1} | Avg L1 Loss: {avg_loss:.6f} | Avg SSIM: {avg_ssim:.4f}\")\n",
    "\n",
    "if avg_ssim > best_ssim:\n",
    "        best_ssim = avg_ssim\n",
    "        torch.save(student.state_dict(), \"best_student.pth\")\n",
    "        print(f\" Best model saved at Epoch {epoch+1} with SSIM: {best_ssim:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
